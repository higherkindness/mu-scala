// When the user clicks on the search box, we want to toggle the search dropdown
function displayToggleSearch(e) {
  e.preventDefault();
  e.stopPropagation();

  closeDropdownSearch(e);
  
  if (idx === null) {
    console.log("Building search index...");
    prepareIdxAndDocMap();
    console.log("Search index built.");
  }
  const dropdown = document.querySelector("#search-dropdown-content");
  if (dropdown) {
    if (!dropdown.classList.contains("show")) {
      dropdown.classList.add("show");
    }
    document.addEventListener("click", closeDropdownSearch);
    document.addEventListener("keydown", searchOnKeyDown);
    document.addEventListener("keyup", searchOnKeyUp);
  }
}

//We want to prepare the index only after clicking the search bar
var idx = null
const docMap = new Map()

function prepareIdxAndDocMap() {
  const docs = [  
    {
      "title": "RPC service definition with Avro",
      "url": "/mu-scala/tutorials/service-definition/avro",
      "content": "Tutorial: RPC service definition with Avro gRPC supports Protocol Buffers by default for serialization of requests and responses, but it also allows you to use other serialization mechanisms, including Avro. This tutorial will show you how to generate a Mu RPC service definition from an Avro IDL file. Then a follow-up tutorial will guide you through using this service definition to create a fully working gRPC server or client. This tutorial is aimed at developers who: are new to Mu-Scala have some understanding of Avro and .avdl (Avro IDL) syntax have read the Getting Started guide This document will focus on Avro. If you would like to use gRPC with Protobuf, see the RPC service definition with Protobuf tutorial. Create a new Mu project As described in the Getting Started guide, we recommend you use the Mu-Scala giter8 template to create a new skeleton project. This will install and configure the mu-srcgen sbt plugin, which we will need to generate Scala code from an Avro .avdl file. When you create the project using sbt new, make sure to set create_sample_code to no. That way you can start with an empty project, and gradually fill in the implementation as you follow the tutorial. You should also set use_proto to no, and use_avro to yes. This will ensure the sbt project is correctly configured to generate code from Avro IDL files. Write the IDL file We’re going to start by writing a .avdl file containing a couple of messages. These messages will be used as the request and response types for a gRPC endpoint later. Copy the following Avro IDL and save it as protocol/src/main/resources/avro/hello.avdl: @namespace(\"com.example\") protocol Greeter { record HelloRequest { string name; } record HelloResponse { string greeting; boolean happy; } } Generate Scala code Now we have a .avdl file, we can generate Scala code from it. Start sbt and run the muSrcGen task. This will discover the .avdl file, parse it and generate corresponding Scala code. Let’s have a look at the code that Mu-Scala has generated. Open the file protocol/target/scala-2.13/src_managed/main/com/example/Greeter.scala in your editor of choice. It should look something like this: package com.example import higherkindness.mu.rpc.internal.encoders.avro.bigDecimalTagged._ import higherkindness.mu.rpc.internal.encoders.avro.javatime._ import higherkindness.mu.rpc.protocol._ final case class HelloRequest(name: String) final case class HelloResponse(greeting: String, happy: Boolean) A few things to note: Mu-Scala has generated one case class for each Avro record The package name matches the namespace specified in the .avdl file Add an RPC service We now have some model classes to represent our RPC request and response, but we don’t have any RPC endpoints. Let’s fix that by adding a method to the Avro protocol. Add the following line to hello.avro to define an RPC endpoint: HelloResponse SayHello(HelloRequest request); Make sure that you add the line inside the protocol block, before the closing curly brace. Regenerate the code If you run the muSrcGen sbt task again, and inspect the protocol/target/scala-2.13/src_managed/main/com/example/Greeter.scala file again, it should look something like this: package com.example import higherkindness.mu.rpc.internal.encoders.avro.bigDecimalTagged._ import higherkindness.mu.rpc.internal.encoders.avro.javatime._ import higherkindness.mu.rpc.protocol._ final case class HelloRequest(name: String) final case class HelloResponse(greeting: String, happy: Boolean) @service(Avro, compressionType = Identity, namespace = Some(\"com.example\")) trait Greeter[F[_]] { def SayHello(request: com.example.HelloRequest): F[com.example.HelloResponse] } Now that our .avdl file has at least one method, we have an RPC service definition, so a corresponding trait has been added to the generated code. There’s quite a lot going on there, so let’s unpack it a bit. The trait is called Greeter, which matches the protocol name in the .avdl file. The trait contains a method for each endpoint in the service. Mu-Scala uses “tagless final” encoding: the trait has a higher-kinded type parameter F[_] and all methods return their result wrapped in F[...]. As we’ll see in a later tutorial, F[_] becomes an IO monad such as cats-effect IO when we implement a gRPC server or client. The trait is annotated with @service. This is a macro annotation. When we compile the code, it will create a companion object for the trait containing a load of useful helper methods for creating servers and clients. We’ll see how to make use of these helpers in the next tutorial. The annotation has 3 parameters: Avro describes how gRPC requests and responses are serialized Identity means GZip compression of requests and responses is disabled \"com.example\" is the namespace in which the RPC endpoint will be exposed These parameters can be customised using sbt settings. Take a look at the source generation reference for more details. Next steps To find out how to turn this service definition into a working gRPC client or server, continue to the gRPC server and client tutorial."
    } ,    
    {
      "title": "Schema Evolution - Avro",
      "url": "/mu-scala/reference/schema-evolution/avro",
      "content": "Avro - Schema Evolution From now on, consider that we are using AvroWithSchema as the serialization mechanism in your Mu program. According to the Avro Specs: A reader of Avro data, whether from an RPC or a file, can always parse that data because its schema is provided. But that schema may not be exactly the schema that was expected. For example, if the data was written with a different version of the software than it is read, then records may have had fields added or removed. For Scala, this section specifies how such schema differences should be resolved to preserve compatibility. We’ll try to summarise a bit all the possible cases in both ends: request and response. However, you could go deeper by using this repo where you can play with all of the possibilities. Cases Modifying the Request (Client side) A: Adding a new non-optional field B: Adding a new optional field C: Adding new item to a union D: Removing item from a union E: Replacing item in a union F: Changing the type of an existing field G: Renaming a field H: Removing a field Modifying the Response (Server side) I: Adding a new non-optional field J: Adding a new optional field K: Adding a new item to a union L: Removing item from a union M: Replacing item from a union N: Changing the type of an existing field O: Renaming a field P: Removing a field Modifying the Request (Client side) A: Adding a new non-optional field You need to specify a default value for the new field. Before: case class Request(a: String, b: Int) After: case class NewRequest(a: String, b: Int, c: Boolean = true) B: Adding a new optional field This is a particular case of the previous scenario, hence, the solution could be providing a default value, that presumably, in Scala would be None. C: Adding new item to a union In this case, we are safe and no actions are required. Before: case class Request(a: Int :+: String :+: CNil) After: case class NewRequest(a: Int :+: String :+: Boolean :+: CNil) D: Removing item from a union In this case, we’d be breaking the compatibility. The only way to deal with this situation is considering this as a change of type, see the next case for details. Before: case class Request(a: Int :+: String :+: CNil) After: case class NewRequest(b: String :+: CNil = Coproduct[String :+: CNil](\"\")) E: Replacing item in a union As we saw previously, again we are breaking the compatibility. If the type is replaced, it will work while the provided value is one of the types in common between the previous coproduct and the new one. Before: case class Request(a: Int :+: String :+: CNil) After: case class NewRequest(a: Int :+: Boolean :+: CNil) It will work if the request is: Request(a = Coproduct[Int :+: String :+: CNil](10)) And it will fail if the request is: Request(a = Coproduct[Int :+: String :+: CNil](\"Hi\")) F: Changing the type of an existing field In this case, it’s not possible to deal with a type swap, if we are maintaining the same name of the field. So the solution would be to consider this replacement as a combination of removing the old field/type, and adding a new field with the new type with a default value. Before: case class Request(a: String, b: Int) After: case class NewRequest(a: String, c: Boolean = true) G: Renaming a field This operation is completely safe, in Avro, the names are not being sent as part of the request, so no matter how they are named. Before: case class Request(a: String, b: Int) After: case class NewRequest(a: String, c: Int) H: Removing a field No action required. However, keep in mind that the value will be ignored when old clients include it in the request. Before: case class Request(a: String, b: Int) After: case class NewRequest(a: String) Modifying the Response (Server side) I: Adding a new non-optional field In this case, the old clients will ignore the value of the new field, so everything will be safe in terms of backward compatibility. Before: case class Response(a: String, b: Int) After: case class NewResponse(a: String, b: Int, c: Boolean) J: Adding a new optional field This would be just a particular case of the previous scenario, where the default value would be None, in the case of Scala. K: Adding a new item to a union In this scenario, the old clients will fail when the result is including the new item. Hence, the solution would be to provide a default value to the old coproduct and creating a new field with the new coproduct. Before: case class Response(a: Int :+: String :+: CNil) After: case class NewResponse( a: Int :+: String :+: CNil = Coproduct[Int :+: String :+: CNil](0), b: Int :+: String :+: Boolean :+: CNil) L: Removing item from a union No action will be required in this case. Before: case class Response(a: Int :+: String :+: CNil) After: case class NewResponse(a: Int :+: CNil) M: Replacing item from a union As long as the value of the coproduct belongs to the previous version, the old client should be able to accept the response as valid. Thus, we would need to follow the same approach as above when Adding a new item to a coproduct. Before: case class Response(a: Int :+: String :+: CNil) After: case class NewResponse(a: Int :+: Boolean :+: CNil) N: Changing the type of an existing field It will require providing a default value for the previous type, and then, we would need to create a new field with the new type. Before: case class Response(a: String, b: Int) After: case class NewResponse(a: String, b: Int = 123, c: Boolean) O: Renaming a field It’s also safe in Avro, the server responses don’t include the parameter names inside and they will be ignored when the data is serialized and sent through the wire. Before: case class Response(a: String, b: Int) After: case class NewResponse(a: String, c: Int) P: Removing a field This evolution should never happen since we would lose backward compatibility. Nonetheless, it would work only under the special case that the old response has a default value for the field that we want to delete, where this operation would be feasible by removing the field in the new version of the server response. Before: case class Response(a: String, b: Int = 123) After: case class NewResponse(a: String)"
    } ,    
    {
      "title": "Generating sources from Avro",
      "url": "/mu-scala/guides/generate-sources-from-avro",
      "content": "Generating sources from Avro Getting started First add the sbt plugin in project/plugins.sbt: addSbtPlugin(\"io.higherkindness\" % \"sbt-mu-srcgen\" % \"0.27.2\") NOTE For users of the sbt-mu-srcgen plugin v0.22.x and below, the plugin is enabled automatically as soon as it’s added to the project/plugins.sbt. However, for users of the sbt-mu-srcgen plugin v0.23.x and beyond, the plugin needs to be manually enabled for any module for which you want to generate code. To enable the module, add the following line to your build.sbt enablePlugins(SrcGenPlugin) Once the plugin is enabled, you can configure it by adding a few lines to build.sbt: import higherkindness.mu.rpc.srcgen.Model._ // Look for Avro IDL files muSrcGenIdlType := IdlType.Avro Finally, make sure you have Scala macro annotations enabled, to ensure the generated code compiles. How you do this depends on which Scala version you are using. For Scala 2.12, add this to build.sbt: addCompilerPlugin(\"org.scalamacros\" % \"paradise\" % \"2.1.1\" cross CrossVersion.patch) For Scala 2.13, add this: scalacOptions += \"-Ymacro-annotations\" Suppose you want to generate Scala code for a gRPC service based on the following Avro IDL file, src/main/resources/hello.avdl: @namespace(\"foo\") protocol AvroGreeter { record HelloRequest { string arg1; union { null, string } arg2; array&lt;string&gt; arg3; } record HelloResponse { string arg1; union { null, string } arg2; array&lt;string&gt; arg3; } foo.HelloResponse sayHelloAvro(foo.HelloRequest arg); } NOTE: please be aware that mu-scala restricts Avro RPC method arguments to a single record type and only permits records as return types; for more context, see the source generation reference. You can run the source generator directly: ```shell script sbt muSrcGen or as part of compilation: ```shell script sbt compile Once the source generator has run, there should be a generated Scala file at target/scala-2.13/src_managed/main/foo/AvroGreeter.scala. It will look like this (tidied up and simplified for readability): package foo import higherkindness.mu.rpc.internal.encoders.avro.bigDecimalTagged._ import higherkindness.mu.rpc.internal.encoders.avro.javatime._ import higherkindness.mu.rpc.protocol._ final case class HelloRequest( arg1: String, arg2: Option[String], arg3: List[String] ) final case class HelloResponse( arg1: String, arg2: Option[String], arg3: List[String] ) @service(Avro, compressionType = Identity, namespace = Some(\"foo\")) trait AvroGreeter[F[_]] { def sayHelloAvro(arg: HelloRequest): F[HelloResponse] } It’s also possible to generate Scala code from .avpr (JSON) files. Suppose you delete src/main/resources/hello.avdl and replace it with src/main/resources/hello.avpr: { \"namespace\" : \"foo\", \"protocol\" : \"AvroGreeter\", \"types\" : [ { \"name\" : \"HelloRequest\", \"type\" : \"record\", \"fields\" : [ { \"name\" : \"arg1\", \"type\" : \"string\" }, { \"name\" : \"arg2\", \"type\" : [ \"null\", \"string\" ] }, { \"name\" : \"arg3\", \"type\" : { \"type\" : \"array\", \"items\" : \"string\" } } ] }, { \"name\" : \"HelloResponse\", \"type\" : \"record\", \"fields\" : [ { \"name\" : \"arg1\", \"type\" : \"string\" }, { \"name\" : \"arg2\", \"type\" : [ \"null\", \"string\" ] }, { \"name\" : \"arg3\", \"type\" : { \"type\" : \"array\", \"items\" : \"string\" } } ] } ], \"messages\" : { \"sayHelloAvro\" : { \"request\" : [ { \"name\" : \"arg\", \"type\" : \"HelloRequest\" } ], \"response\" : \"HelloResponse\" } } } If you run sbt clean muSrcGen, you should end up with exactly the same generated Scala file as before. Avro code generation details This section explains the different Scala structures that are generated from Avro IDL. To achieve this generation Mu’s source generator uses avrohugger behind the scenes. Avro Protocols Let’s start from the beginning, everything on Avro should be declared inside a protocol. The name of that protocol will be the name of our Scala file. protocol People { ... } muSrcGen =&gt; People.scala Furthermore, the protocol can have a namespace which will be our Scala package: @namespace(\"example.protocol\") protocol People { ... } muSrcGen =&gt; example.protocol.People.scala Messages On Avro, the messages are declared with the keyword record and contains different fields inside. The record will be translated to a case class with the same fields on it: record Person { string name; int age; boolean crossfitter; } muSrcGen =&gt; case class Person(name: String, age: Int, crossfitter: Boolean) Enums Avro supports enums too and they are translated to a Scala Enumeration: enum Errors { NotFound, Duplicated, None } muSrcGen =&gt; object Errors extends Enumeration { type Errors = Value val NotFound, Duplicated, None = Value } Unions Unions are a complex Avro type for fields inside records. As its name suggest, it represents a type composed by another types. Depending on the types composing the union, Mu will interpret it on different ways: Optional fields When we add a null to a union expression, we’ll get a Scala Option of the other types declared along the null: record PeopleRequest { union {null, string} name; } muSrcGen =&gt; case class PeopleRequest(name: Option[String]) Eithers When we join two non-null types on a union we’ll get an Scala Either with the same types order: record PeopleResponse { union { Errors, Person } result; } muSrcGen =&gt; case class PeopleResponse(result: Either[Errors.Value, Person]) Coproducts And finally, when we have three or more non-null types on a single union, we’ll have a shapeless’ Coproduct on the same order as well: record PeopleResponse { union{ string, int, Errors } result; } muSrcGen =&gt; import shapeless.{:+:, CNil} case class PeopleResponse(result: String :+: Int :+: Errors.Value :+: CNil) Services When we declare a method or endpoint inside a protocol this will be converted to a trait and intended as a Mu service. As we would want to have our models separated from our services. Avro make us able to import other Avro files to use their records: protocol PeopleService { import idl \"People.avdl\"; //Under the same folder example.protocol.PeopleResponse getPerson(example.protocol.PeopleRequest request); } muSrcGen =&gt; @service(Avro) trait PeopleService[F[_]] { def getPerson(request: example.protocol.PeopleRequest): F[example.protocol.PeopleResponse] } Also, an endpoint can be declared without params or non returning anything and Mu will use its Empty type to cover these cases: protocol PeopleService { void insertPerson(); } muSrcGen =&gt; @service(Avro) trait PeopleService[F[_]] { def insertPerson(arg: Empty.type): F[Empty.type] } For a full understanding of the Avro syntax we recommend you to take a look to the Avro Official site where you can find all the Avro supported types and some interesting resources."
    } ,    
    {
      "title": "Generating sources from IDLs stored in Compendium",
      "url": "/mu-scala/guides/generate-sources-from-compendium",
      "content": "Generating sources from IDLs stored in Compendium compendium is a standalone solution, implemented as an HTTP service, that provides storage, conversion and client generation for your schemas in a format-agnostic fashion. sbt-mu-srcgen provides a feature that enables the user to store and get IDL definitions from compendium. This section will provide instructions about how to configure the sbt settings to use compendium. We are assuming you have a compendium instance running. If not, please check the compendium microsite. Also, the configuration related to the IDL type could be checked on: Avro section Protobuf section Configure sbt plugin Settings related to compendium interaction are: Setting Type Description Default value muSrcGenExecutionMode ExecutionMode Execution mode of the plugin. If Compendium, it’s required a compendium instance where IDL files are saved. Local muSrcGenCompendiumServerUrl String Compendium server url http://localhost:8080 muSrcGenCompendiumProtocolIdentifiers Seq[ProtocolAndVersion] Protocol identifiers to retrieve from compendium. ProtocolAndVersion provides two values: name (mandatory) that corresponds with the identifier used to store the protocol and version (optional) Nil So in order to use compendium, you’ll need to add those settings in your build.sbt: muSrcGenCompendiumProtocolIdentifiers := List(ProtocolAndVersion(\"example1\", None)) muSrcGenExecutionMode := Compendium If needed, you can also point out to your compendium server if it’s not on the default value, http://localhost:8080: muSrcGenCompendiumServerUrl := \"http://localhost:47047\" muSrcGenCompendiumProtocolIdentifiers := List(ProtocolAndVersion(\"example2\", Some(2))) muSrcGenExecutionMode := Compendium"
    } ,    
    {
      "title": "Custom gRPC Serialization",
      "url": "/mu-scala/guides/custom-grpc-serialization",
      "content": "Custom gRPC serialization Mu will serialize gRPC requests and responses using Avro or Protobuf, depending on the compression format specified in the @service annotation on the service definition trait. This serialization can be customised in a few different ways. Compression Mu supports compression of RPC requests and responses. We can enable this compression either on the server or the client side. Mu supports Gzip as the compression format. The server will automatically handle compressed requests from clients, decompressing them appropriately. To make the server compress its responses, set the compression type argument to Gzip in the @service annotation when defining the service. For example: import higherkindness.mu.rpc.protocol._ object CompressionExample { case class HelloRequest(name: String) case class HelloResponse(greeting: String) @service(Protobuf, compressionType = Gzip) trait Greeter[F[_]] { def emptyCompressed(req: HelloRequest): F[HelloResponse] } } The client will automatically handle compressed responses from servers, decompressing them appropriately. To make the client compress its requests, you need to add the appropriate “call option” when constructing the client. Here is an example of a client with request compression enabled. import cats.effect._ import higherkindness.mu.rpc._ import io.grpc.CallOptions import CompressionExample._ object CompressionExampleClient { val channelFor: ChannelFor = ChannelForAddress(\"localhost\", 12345) def clientResource[F[_]: Async]: Resource[F, Greeter[F]] = Greeter.client[F](channelFor, options = CallOptions.DEFAULT.withCompression(\"gzip\")) } Technical details To be strictly accurate, when you enable compression on the client or server side, the requests and responses are not compressed, but the messages inside them are. For example, if you enable compression on the client side, the client will compress the message when constructing a request. It will set the compression flag on the message to indicate that it is compressed, and it will set the grpc-encoding: gzip request header so that the server knows how to decompress the message. Custom codecs Mu allows you to use custom decoders and encoders for Avro/Protobuf serialization of gRPC requests and responses. Let’s look at an example in both Avro and Protobuf. Custom Protobuf codec Mu uses a library called PBDirect for Protobuf serialization. To customise the serialization of fields in Protobuf messages, you need to provide instances of PBDirect’s PBScalarValueReader and PBScalarValueWriter type classes. Here is an example of providing a custom reader and writer for java.time.LocalDate that serializes the date as a String in ISO 8601 format. We create the reader and writer by building on PBDirect’s built-in reader and writer for String, using map and contramap respectively. object ProtobufCustomCodecExample { import java.time._ import java.time.format._ import pbdirect._ import cats.syntax.contravariant._ import cats.syntax.functor._ implicit val localDateReader: PBScalarValueReader[LocalDate] = PBScalarValueReader[String].map(string =&gt; LocalDate.parse(string, DateTimeFormatter.ISO_LOCAL_DATE)) implicit val localDateWriter: PBScalarValueWriter[LocalDate] = PBScalarValueWriter[String].contramap[LocalDate](_.format(DateTimeFormatter.ISO_LOCAL_DATE)) case class HelloRequest(name: String, date: LocalDate) case class HelloReply(message: String) @service(Protobuf) trait Greeter[F[_]] { def sayHello(request: HelloRequest): F[HelloReply] } } Custom Avro codec Mu uses a library called avro4s For Avro serialization. To customise the serialization of fields in Avro records, you need to provide instances of three avro4s type classes: SchemaFor, Encoder, and Decoder. Let’s look at the same example as above, this time for Avro. object AvroCustomCodecExample { import java.time._ import java.time.format._ import com.sksamuel.avro4s._ import org.apache.avro.Schema implicit val LocalDateSchemaFor: SchemaFor[LocalDate] = SchemaFor[LocalDate](Schema.create(Schema.Type.STRING)) implicit object LocalDateEncoder extends Encoder[LocalDate] { override val schemaFor = LocalDateSchemaFor override def encode(value: LocalDate): String = value.format(DateTimeFormatter.ISO_LOCAL_DATE) } implicit object LocalDateDecoder extends Decoder[LocalDate] { override val schemaFor = LocalDateSchemaFor override def decode(value: Any): LocalDate = LocalDate.parse(value.toString(), DateTimeFormatter.ISO_LOCAL_DATE) } case class HelloRequest(name: String, date: LocalDate) case class HelloReply(message: String) @service(Avro) trait Greeter[F[_]] { def sayHello(request: HelloRequest): F[HelloReply] } } Protobuf codecs Mu provides Protobuf codecs for: BigDecimal java.time.LocalDate, java.time.LocalDateTime and java.time.Instant Add the following imports to your service code: Types Import BigDecimal import higherkindness.mu.rpc.internal.encoders.pbd.bigDecimal._ java.time.{LocalDate, LocalDateTime, Instant} import higherkindness.mu.rpc.internal.encoders.pbd.javatime._ Avro codecs Mu provides Avro codecs for: BigDecimal java.time.LocalDate, java.time.LocalDateTime and java.time.Instant Add the following imports to your service code: Types Import BigDecimal import higherkindness.mu.rpc.internal.encoders.avro.bigDecimal._ java.time.* import higherkindness.mu.rpc.internal.encoders.avro.javatime._ Notes: If you want to send one of these types directly as an Avro-encoded request or response (instead of as a field within a request or response), you need to provide an instance of io.grpc.MethodDescriptor.Marshaller. Mu provides marshallers for these types under separate imports: Types Import BigDecimal import higherkindness.mu.rpc.internal.encoders.avro.bigDecimal.marshallers._ java.time.* import higherkindness.mu.rpc.internal.encoders.avro.javatime.marshallers._"
    } ,    
    {
      "title": "Deploying a Mu service",
      "url": "/mu-scala/tutorials/deployment",
      "content": "Tutorial: Deploying a Mu service We don’t have documentation for this yet, but you may find this article useful: Mu in the cloud. It goes into significant detail, explaining how to deploy an example Mu application to Kubernetes in Google Cloud Platform. All of the code for that article is also available on GitHub for reference."
    } ,    
    {
      "title": "Distributed Tracing",
      "url": "/mu-scala/guides/distributed-tracing",
      "content": "Distributed Tracing Mu provides an integration with Natchez to enable distributed tracing of gRPC calls. Specifically, the integration provides the following features. Client For every RPC call, the client will create a child span with the fully qualified name of the RPC method being called (e.g. com.foo.MyService/SayHello) It will automatically add all necessary trace-related headers to RPC requests. Server The server will attempt to extract trace-related information from the request headers. It will create a span using the same naming convention as the client. If the relevant headers were present, it will continue the trace that was started upstream, creating a child span. Otherwise, it will create a root span, i.e. a new trace. How to use Let’s look at how to enable tracing on the server side first. Server side We’ll assume the following service definition: import higherkindness.mu.rpc.protocol._ case class HelloRequest(name: String) case class HelloResponse(greeting: String) @service(Protobuf, namespace = Some(\"com.foo\")) trait MyService[F[_]] { def SayHello(req: HelloRequest): F[HelloResponse] } and an implementation of that definition: import cats.Applicative import cats.syntax.applicative._ class MyAmazingService[F[_]: Applicative] extends MyService[F] { def SayHello(req: HelloRequest): F[HelloResponse] = HelloResponse(s\"Hello, ${req.name}!\").pure[F] } Ordinarily, if you were not using tracing, you would create a gRPC service definition using the macro-generated MyService.bindService method, specifying your effect monad of choice: import cats.effect.{IO, IOApp, ExitCode} import higherkindness.mu.rpc.server.{GrpcServer, AddService} object OrdinaryServer extends IOApp { implicit val service: MyService[IO] = new MyAmazingService[IO] def run(args: List[String]): IO[ExitCode] = (for { serviceDef &lt;- MyService.bindService[IO] _ &lt;- GrpcServer.defaultServer[IO](8080, List(AddService(serviceDef))) } yield ()).useForever } To use the same service with tracing enabled, you need to call the MyService.bindTracingService method instead. bindTracingService[F[_]] differs from bindService[F[_]] in two ways, which we will explain below. It takes a Natchez EntryPoint as an argument. It takes a MyService as an implicit argument, but instead of a MyService[F] it requires a MyService[Kleisli[F, Span[F], *]]. EntryPoint EntryPoint[F[_]], as the name suggests, is the “entrypoint” into the Natchez API. It’s what allows Mu to do things like create root spans. How you create an EntryPoint will depend on what tracing implementation you want to use. For example, if you use natchez-jaeger, you might create a Resource of an EntryPoint like this: import cats.effect.{Sync, Resource} import natchez.EntryPoint import natchez.jaeger.Jaeger import io.jaegertracing.Configuration.SamplerConfiguration import io.jaegertracing.Configuration.ReporterConfiguration def entryPoint[F[_]: Sync]: Resource[F, EntryPoint[F]] = { Jaeger.entryPoint[F](\"my-Mu-service\") { c =&gt; Sync[F].delay { c.withSampler(SamplerConfiguration.fromEnv) .withReporter(ReporterConfiguration.fromEnv) .getTracer } } } Kleisli When you instantiate your MyService implementation, you need to set its type parameter to Kleisli[F, Span[F], *]. (Note: we are using kind-projector syntax here, but you don’t have to.) Intuitively, this creates a service which, given the current span as input, returns a result inside the F effect. Luckily, there are instances of most of the cats-effect type classes for Kleisli, all the way down to Concurrent (but not Effect). So you should be able to substitute MyService[Kleisli[F, Span[F], *]] for MyService[F] without requiring any changes to your service implementation code. Using bindTracingService Putting all this together, your server setup code will look something like this: import cats.data.Kleisli import natchez.Span object TracingServer extends IOApp { implicit val service: MyService[Kleisli[IO, Span[IO], *]] = new MyAmazingService[Kleisli[IO, Span[IO], *]] def run(args: List[String]): IO[ExitCode] = entryPoint[IO] .flatMap { ep =&gt; MyService.bindTracingService[IO](ep) } .flatMap { serviceDef =&gt; GrpcServer.defaultServer[IO](8080, List(AddService(serviceDef))) }.useForever } Tracing your service code If you wish, you can make use of the Natchez Trace typeclass to create child spans: import natchez.Trace import cats.Monad import cats.syntax.all._ class MyTracingService[F[_]: Monad: Trace] extends MyService[F] { def SayHello(req: HelloRequest): F[HelloResponse] = for { _ &lt;- Trace[F].span(\"look stuff up in the database\"){ Monad[F].unit } _ &lt;- Trace[F].span(\"do some stuff with Redis\"){ Monad[F].unit } _ &lt;- Trace[F].span(\"make an HTTP call\"){ Monad[F].unit } } yield HelloResponse(s\"Hi, ${req.name}!\") } Client side Ordinarily, if you were not using tracing, you would create a cats-effect Resource of an RPC client using the macro-generated MyService.client method: import higherkindness.mu.rpc.{ChannelFor, ChannelForAddress} object OrdinaryClientApp extends IOApp { val channelFor: ChannelFor = ChannelForAddress(\"localhost\", 8080) val clientRes: Resource[IO, MyService[IO]] = MyService.client[IO](channelFor) def run(args: List[String]): IO[ExitCode] = clientRes.use { client =&gt; for { resp &lt;- client.SayHello(HelloRequest(\"Chris\")) _ &lt;- IO(println(s\"Response: $resp\")) } yield (ExitCode.Success) } } To obtain a tracing client, use MyService.tracingClient instead of MyService.client. This returns a MyService[Kleisli[F, Span[F], *]], i.e. a client which takes the current span as input and returns a response inside the F effect. For example: object TracingClientApp extends IOApp { val channelFor: ChannelFor = ChannelForAddress(\"localhost\", 8080) val clientRes: Resource[IO, MyService[Kleisli[IO, Span[IO], *]]] = MyService.tracingClient[IO](channelFor) def run(args: List[String]): IO[ExitCode] = entryPoint[IO].use { ep =&gt; ep.root(\"this is the root span\").use { currentSpan =&gt; clientRes.use { client =&gt; val kleisli = client.SayHello(HelloRequest(\"Chris\")) for { resp &lt;- kleisli.run(currentSpan) _ &lt;- IO(println(s\"Response: $resp\")) } yield (ExitCode.Success) } } } } Working example To see a full working example of distributed tracing across multiple Mu services, take a look at this repo: cb372/mu-tracing-example. The README explains how to run the example and inspect the resulting traces."
    } ,    
    {
      "title": "Further reading",
      "url": "/mu-scala/reference/further-reading",
      "content": "Useful links Higherkindness Skeuomorph RPC gRPC Avro Protocol Buffers Docs scalamacros PBDirect Monix FS2 Docs gRPC Java API Metrifier HTTP/2 Comparing HTTP and RPC This is not specifically about Mu. Very often our microservices architectures are based on HTTP with JSON serialization, but perhaps it is not the best glue to connect them, and RPC services might be a better fit. Metrifier is a project where we compare, in different bounded ecosystems, HTTP and RPC. We found that RPC is usually faster than HTTP + JSON. If you’re interested in learning more, we encourage you to take a look at the documentation. Next Steps If you want to dive deeper into Mu, we have a complete example in the examples repo, which is based on the Route Guide Demo originally shared by the gRPC Java Project."
    } ,    
    {
      "title": "Getting Started",
      "url": "/mu-scala/getting-started",
      "content": "Getting Started The easiest way to get started is to use our giter8 template to start a new project: shell script sbt new higherkindness/mu-scala.g8 You can customise the template using a few parameters: The template will generate an sbt project with 3 modules: the protocol module, for generating source code from Avro/Protobuf IDL files the server module, for a gRPC server the client module, for a gRPC client Template parameters There are a few important parameters to note. create_sample_code If you set the create_sample_code parameter to yes (the default value), the project will include sample code demonstrating how to build a gRPC server and client with Mu: the protocol module will contain a “hello world” Avro/Protobuf IDL file the client module will contain a working implementation of a gRPC client the server module will contain a working implementation of a gRPC server, as well as a unit test If you set create_sample_code to no, the three modules will still be created, but they will be empty. use_protobuf and use_avro You should set exactly one of these to yes, and the other to no. Depending on these parameters, the example IDL file created in the protocol module will be either a .proto or a .avdl file. Try it out If you chose to create sample code, you can see everything working: Start the server with sbt server/run In another terminal window, run the client with sbt client/run and enter your name when prompted You should see something like this: You can also run the unit test with sbt server/test. Next steps Learn more about Mu-Scala concepts by following a tutorial. The RPC service definition with Protobuf tutorial, or RPC service definition with Avro if you prefer Avro, is a good place to start."
    } ,    
    {
      "title": "gRPC server and client",
      "url": "/mu-scala/tutorials/grpc-server-client",
      "content": "Tutorial: gRPC server and client This tutorial will show you how to implement a working gRPC server and client based on a Mu service defintion. This tutorial is aimed at developers who: are new to Mu-Scala have some understanding of cats-effect have read the Getting Started guide have followed either the RPC service definition with Protobuf or RPC service definition with Avro tutorial Mu supports both Protobuf and Avro. For the purposes of this tutorial we will assume you are using Protobuf, but it’s possible to follow the tutorial even if you are using Avro. Service definition If you have followed one of the previous tutorials, you should already have a service definition that looks like this: import higherkindness.mu.rpc.protocol._ object hello { case class HelloRequest(@pbdirect.pbIndex(1) name: String) case class HelloResponse(@pbdirect.pbIndex(1) greeting: String, @pbdirect.pbIndex(2) happy: Boolean) // Note: the @service annotation in your code might reference Avro instead of Protobuf @service(Protobuf, compressionType = Identity, namespace = Some(\"com.example\")) trait Greeter[F[_]] { def SayHello(req: HelloRequest): F[HelloResponse] } } Implement the server This is the interesting part: writing the business logic for your service. We do this by implementing the Greeter trait. Let’s make a Greeter that says “hello” in a happy voice: import cats.Applicative import cats.syntax.applicative._ import hello._ class HappyGreeter[F[_]: Applicative] extends Greeter[F] { def SayHello(req: HelloRequest): F[HelloResponse] = HelloResponse(s\"Hello, ${req.name}!\", happy = true).pure[F] } Note that in this implementation we aren’t performing any effects, so we don’t care what F[_] is as long as we can lift a pure value into it. Server entrypoint Now we have a Greeter implementation, let’s expose it as a gRPC server. We’re going to use cats-effect IO as our concrete IO monad, and we’ll make use IOApp from cats-effect. import cats.effect.{IO, IOApp, ExitCode, Resource} import hello.Greeter import higherkindness.mu.rpc.server.{GrpcServer, AddService} object Server extends IOApp { implicit val greeter: Greeter[IO] = new HappyGreeter[IO] // 1 def run(args: List[String]): IO[ExitCode] = (for { serviceDef &lt;- Greeter.bindService[IO] // 2 server &lt;- Resource.eval(GrpcServer.default[IO](12345, List(AddService(serviceDef)))) // 3 _ &lt;- GrpcServer.serverResource[IO](server) // 4 } yield ()).useForever } Let’s go through this line by line. First we instantiate our HappyGreeter, concretized to IO, and make it available implicitly for use by Greeter.bindService. Next we call Greeter.bindService. This is a helper method generated by the @service macro annotation on the Greeter trait. It converts our Greeter service into a gRPC “service definition”, returning IO[io.grpc.ServerServiceDefinition]. Each Scala method in the service will become a gRPC method of the same name, with the following adjustments: If the @service annotation has a methodNameStyle = Capitalize argument, the first letter of the method name will be capitalized; If the @service annotation has a namespace = Some(ns) argument, the method name will be prefixed by the value of ns, followed by a dot. We build a description of the whole gRPC server by calling GrpcServer.default. We tell it the port we want to run on (12345), and the list of services we want it to expose. The method is called default because we want to use gRPC’s default HTTP transport layer. Finally we can call GrpcServer.serverResource, passing it our server description. This actually starts the server. If you copy the above code into a .scala file in the server module of your project, you should be able to start a server using sbt server/run. Client Let’s see how to make a client to communicate with the server. Here is a tiny demo that makes a request to the SayHello endpoint and prints out the reply to the console. import cats.effect.{IO, IOApp, Resource, ExitCode} import hello.{Greeter, HelloRequest} import higherkindness.mu.rpc._ object ClientDemo extends IOApp { val channelFor: ChannelFor = ChannelForAddress(\"localhost\", 12345) // 1 val clientResource: Resource[IO, Greeter[IO]] = Greeter.client[IO](channelFor) // 2 def run(args: List[String]): IO[ExitCode] = for { response &lt;- clientResource.use(c =&gt; c.SayHello(HelloRequest(name = \"Chris\"))) // 3 serverMood = if (response.happy) \"happy\" else \"unhappy\" _ &lt;- IO(println(s\"The $serverMood server says '${response.greeting}'\")) } yield ExitCode.Success } Again we’ll go through this line by line. We create a channel, which tells the client how to connect to the server. We call Greeter.client, another helper method generated by the @service macro. This returns a cats-effect Resource, which will take care of safely allocated and cleaning up resources every time we want to use a client. We use the resource, using the resulting client to send a request to the SayHello endpoint and get back a response. If you copy the above code into a .scala file in the client module of your project, and your server is still running, you should be able to see the client in action using sbt client/run. [info] running com.example.ClientDemo The happy server says 'Hello, Chris!' [success] Total time: 1 s, completed 5 Mar 2020, 15:49:03 Next steps If you want to write tests for your RPC service, take a look at the Testing an RPC service tutorial."
    } ,    
    {
      "title": "gRPC Streaming",
      "url": "/mu-scala/guides/grpc-streaming",
      "content": "gRPC Streaming In the tutorials, we only defined gRPC services with so-called “unary” endpoints. This is an endpoint that does not involve streaming. The client sends a single request and receives a single response. gRPC also defines the following kinds of streaming, all of which are supported by Mu. Server streaming RPC: similar to the unary service, but in this case the server will send back a stream of responses for a client request. Client streaming RPC: in this case is the client which sends a stream of requests. The server will respond with a single response. Bidirectional streaming RPC: a mix of server and client streaming as both sides will be sending a stream of data. Protobuf Mu only officially supports streaming for Protobuf, not Avro. This is because Avro does not (yet) have support for streaming RPC endpoints in its protocol specification. The relevant Avro issue is AVRO-406. Stream implementation Mu supports both Monix Observable and FS2 Stream for streaming of RPC requests and responses. You can choose whichever data type fits your application’s needs best. Service definition with streaming endpoints Let’s see what a Mu RPC service definition looks like when we introduce streaming endpoints. Before starting, here is one import we’ll need: import higherkindness.mu.rpc.protocol._ And here are the equest/response models for our service: case class HelloRequest(greeting: String) case class HelloResponse(reply: String) We’ll write the service definition using FS2. Using FS2 Let’s write the same service definition using fs2.Stream instead of Observable. object servicefs2 { import fs2.Stream @service(Protobuf) trait Greeter[F[_]] { /** * Server streaming RPC * * @param request Single client request. * @return Stream of server responses. */ def lotsOfReplies(request: HelloRequest): F[Stream[F, HelloResponse]] /** * Client streaming RPC * * @param request Stream of client requests. * @return Single server response. */ def lotsOfGreetings(request: Stream[F, HelloRequest]): F[HelloResponse] /** * Bidirectional streaming RPC * * @param request Stream of client requests. * @return Stream of server responses. */ def bidiHello(request: Stream[F, HelloRequest]): F[Stream[F, HelloResponse]] } }"
    } ,    
    {
      "title": "Tutorials",
      "url": "/mu-scala/tutorials",
      "content": "Tutorials These tutorials are aimed at beginners to Mu-Scala. We recommend you read them in roughly the order they appear in the navigation menu. Before starting the tutorials, we recommend you read the Getting Started guide. If you are already familiar with Mu-Scala, you may want to read about how to use its more advanced features in the How-To Guides."
    } ,    
    {
      "title": "Backward/Forward Data Evolution",
      "url": "/mu-scala/reference/schema-evolution",
      "content": "Backward/Forward Data Evolution This section is about how data flows through the network, and how are they encoded/decoded into/from bytes in both sides of the wire in a compatible way. Currently, Mu brings the ability to encode data in bytes based on Avro and Protocol buffers. In the next sections, we are going to pass through both serialization standards to see how to preserve both forward and backward compatibility in your system: Avro Protocol Buffers"
    } ,    
    {
      "title": "Reference",
      "url": "/mu-scala/reference",
      "content": "Reference This is a collection of reference material about Mu-Scala."
    } ,    
    {
      "title": "Generating sources from IDL",
      "url": "/mu-scala/guides/generate-sources-from-idl",
      "content": "Generate sources from IDL While it is possible to use Mu by hand-writing your service definitions, message classes and clients in Scala, we recommend you use sbt-mu-srcgen to generate this code from Protobuf/Avro/OpenAPI IDL files. IDL files are language-agnostic, more concise than Scala code, easily shared with 3rd parties, and supported by a lot of existing tools. Mu can generate code from a number of different IDL formats: message classes, gRPC server and client from Protobuf .proto files (see Generating sources from Protocol Buffers for detailed instructions) message classes, gRPC server and client from Avro .avpr or .avdl files (see Generating sources from Avro) message classes and REST client from OpenAPI .yaml files (see the OpenAPI REST client tutorial) Plugin Installation Add the following line to project/plugins.sbt: addSbtPlugin(\"io.higherkindness\" % \"sbt-mu-srcgen\" % \"0.27.2\") NOTE For users of the sbt-mu-srcgen plugin v0.22.x and below, the plugin is enabled automatically as soon as it’s added to the project/plugins.sbt. However, for users of the sbt-mu-srcgen plugin v0.23.x and beyond, the plugin needs to be manually enabled for any module for which you want to generate code. To enable the module, add the following line to your build.sbt enablePlugins(SrcGenPlugin) Once the plugin is installed and enabled, you can configure it How to use the plugin The muSrcGen sbt task generates Scala source code from IDL files. The plugin will automatically integrate the source generation into your compile process, so the sources are generated before compilation when you run the compile task. You can also run the sbt task manually: ```shell script sbt muSrcGen ## Import You will need to add this import at the top of your `build.sbt`: ```sbt import higherkindness.mu.rpc.srcgen.Model._ Settings For an explanation of the plugin’s settings, see the source generation reference."
    } ,    
    {
      "title": "How-To Guides",
      "url": "/mu-scala/guides",
      "content": "How-To Guides These guides are aimed at developers who are already familiar with Mu-Scala. If you are new to Mu-Scala, we recommend you read the Getting Started guide and the tutorials. Each guide introduces a single feature of Mu-Scala and explains how to use it."
    } ,    
    {
      "title": "Mu",
      "url": "/mu-scala/",
      "content": "Mu-Scala Mu is a suite of libraries and tools that help you build and maintain microservices and clients in a functional style. Getting Started If you’re new to Mu-Scala, check out the Getting Started guide and the tutorials. Features While you focus on implementing the business logic for your service, let Mu take care of the boilerplate and non-functional requirements, including: generation of model classes, service interfaces and clients from Avro, Protobuf or OpenAPI IDL files serialization of requests and responses into Avro/Protobuf/JSON building high-performance gRPC servers and clients building HTTP REST servers and clients using http4s handling of streaming requests and responses using either FS2 Stream or Monix Observable distributed tracing metrics reporting … and plenty more features on the way! Specifically, Mu helps you to build: gRPC servers and clients based on either Avro or Protobuf protocol definitions REST servers and clients based on OpenAPI definitions"
    } ,      
    {
      "title": "Metrics Reporting",
      "url": "/mu-scala/guides/metrics-reporting",
      "content": "Metrics Reporting Currently, Mu provides two different ways to report metrics about gRPC services: Prometheus and Dropwizard Metrics. The usage is quite similar for both. Mu exposes the following metrics, for both servers and clients: Active calls: number of in-flight messages Messages sent: number of requests sent by the client or responses sent by the server (distributed by service name and method name). Messages received: number of requests received by the server or responses received by the client (distributed by service name and method name). Timers for header calls, total calls, and also distributed by method types (unary, streaming, …) and statuses (ok, canceled, …). Monitor Server Calls In order to monitor the RPC calls on the server side we need two things: A MetricsOps implementation. MetricsOps is an algebra located in the internal-core module with the needed operations for registering metrics. Mu provides two implementations, one for Prometheus and another one for Dropwizard but you can provide your own. A MetricsServerInterceptor. Mu provides an interceptor that receives a MetricsOps as an argument and collects server metrics. Let’s see how to register server metrics using Prometheus in the following fragment. import cats.effect._ import cats.effect.std.Dispatcher import higherkindness.mu.rpc.prometheus.PrometheusMetrics import higherkindness.mu.rpc.server._ import higherkindness.mu.rpc.server.interceptors.implicits._ import higherkindness.mu.rpc.server.metrics.MetricsServerInterceptor import io.prometheus.client.CollectorRegistry import service._ object InterceptingServerCalls { lazy val cr: CollectorRegistry = new CollectorRegistry() implicit val greeterServiceHandler: ServiceHandler[IO] = new ServiceHandler[IO] val server: Resource[IO, GrpcServer[IO]] = for { metricsOps &lt;- Resource.eval(PrometheusMetrics.build[IO](cr, \"server\")) service &lt;- Greeter.bindService[IO] disp &lt;- Dispatcher[IO] withMetrics = service.interceptWith(MetricsServerInterceptor(metricsOps, disp)) server &lt;- Resource.eval(GrpcServer.default[IO](8080, List(AddService(withMetrics)))) } yield server } Monitor Client Calls In this case, in order to intercept the client calls we need additional configuration settings (by using AddInterceptor): import cats.effect.{IO, Resource} import higherkindness.mu.rpc._ import higherkindness.mu.rpc.config._ import higherkindness.mu.rpc.channel._ import higherkindness.mu.rpc.channel.metrics.MetricsChannelInterceptor import higherkindness.mu.rpc.config.channel._ import io.prometheus.client.CollectorRegistry import service._ object InterceptingClientCalls { lazy val cr: CollectorRegistry = new CollectorRegistry() val serviceClient: Resource[IO, Greeter[IO]] = for { channelFor &lt;- Resource.eval(ConfigForAddress[IO](\"rpc.host\", \"rpc.port\")) metricsOps &lt;- Resource.eval(PrometheusMetrics.build[IO](cr, \"client\")) disp &lt;- Dispatcher[IO] serviceClient &lt;- Greeter.client[IO]( channelFor = channelFor, channelConfigList = List(UsePlaintext(), AddInterceptor(MetricsChannelInterceptor(metricsOps, disp)))) } yield serviceClient } That is how we use Prometheus to monitor both gRPC ends. Dropwizard Metrics The usage the same as before, but in this case we need to create a Dropwizard backed MetricsOps import cats.effect.IO import com.codahale.metrics.MetricRegistry import higherkindness.mu.rpc.dropwizard.DropWizardMetrics val registry: MetricRegistry = new MetricRegistry() val metricsOps = DropWizardMetrics[IO](registry) To check the metrics from our server or client, Dropwizard exposes it through JMX. You’ll need the following dependency: \"io.dropwizard.metrics\" % \"metrics-jmx\" % \"4.1.4\" And to associate a JMX reporter with the metrics registry on your project, val jmxReporter = com.codahale.metrics.jmx.JmxReporter.forRegistry(registry) jmxReporter.build().start() More For more details, in metrics integration with Mu you can check a full example about Mu metrics."
    } ,    
    {
      "title": "Modules and artifacts",
      "url": "/mu-scala/reference/modules-artifacts",
      "content": "Mu-Scala modules and artifacts Mu is divided into multiple artifacts, grouped by scope: Server: specifically for RPC servers Client: specifically for RPC clients Server/Client: used from other artifacts for both Server and Client. Test: useful to test Mu applications. RPC Client/Server Artifact Name Scope Mandatory Description mu-rpc-service Server/Client Yes Mandatory to define protocols and auto-derived clients. mu-rpc-monix Server/Client Yes Mandatory to define streaming operations with Monix Observables. mu-rpc-fs2 Server/Client Yes Mandatory to define streaming operations with FS2 Streams. mu-rpc-server Server Yes Needed to attach RPC Services and spin-up an RPC Server. mu-rpc-client-netty Client Yes* Netty transport layer for the client. Mandatory if you need SSL/TLS support. mu-rpc-client-okhttp Client Yes* OkHttp transport layer for the client. An alternative to Netty. mu-rpc-netty-ssl Server/Client No Adds the io.netty:netty-tcnative-boringssl-static:jar dependency, aligned with the Netty version (if that’s the case) used in the mu-rpc build. See this section for more information. By adding this you wouldn’t need to figure the right version, mu-rpc gives you the right one. Yes*: on the client-side, you must choose either Netty or OkHttp as the transport layer. Metrics Artifact Name Scope Mandatory Description mu-rpc-prometheus Server/Client No Interceptors which can be used to monitor gRPC services using Prometheus. mu-rpc-dropwizard Server/Client No Interceptors which can be used to monitor gRPC services using Dropwizard metrics. Other Artifact Name Scope Mandatory Description mu-config Server/Client No Provides configuration helpers using pureconfig to load the application configuration values. mu-rpc-testing Test No Utilities to test out Mu applications. It provides the grpc-testing library as the transitive dependency. mu-rpc-client-cache Client No Provides an algebra for caching RPC clients. Build You can install any of these dependencies in your build as follows: // required for a protocol definition: libraryDependencies += \"io.higherkindness\" %% \"mu-rpc-service\" % \"0.27.2\" // required for a protocol definition with streaming operations: libraryDependencies += \"io.higherkindness\" %% \"mu-rpc-monix\" % \"0.27.2\" // or: libraryDependencies += \"io.higherkindness\" %% \"mu-rpc-fs2\" % \"0.27.2\" // required for the RPC server libraryDependencies += \"io.higherkindness\" %% \"mu-rpc-server\" % \"0.27.2\" // required for the use of generated RPC clients, using either Netty or OkHttp as transport layer: libraryDependencies += \"io.higherkindness\" %% \"mu-rpc-client-netty\" % \"0.27.2\" // or: libraryDependencies += \"io.higherkindness\" %% \"mu-rpc-client-okhttp\" % \"0.27.2\" // optional - for easy RPC server/client configuration. libraryDependencies += \"io.higherkindness\" %% \"mu-config\" % \"0.27.2\" // optional - for RPC server/client metrics reporting, using Prometheus. libraryDependencies += \"io.higherkindness\" %% \"mu-rpc-prometheus\" % \"0.27.2\" // optional - for RPC server/client metrics reporting, using Dropwizard. libraryDependencies += \"io.higherkindness\" %% \"mu-rpc-dropwizard\" % \"0.27.2\" // optional - for communication between RPC server and client using SSL/TLS. libraryDependencies += \"io.higherkindness\" %% \"mu-rpc-netty-ssl\" % \"0.27.2\" // optional - to add caching support to RPC clients. libraryDependencies += \"io.higherkindness\" %% \"mu-rpc-client-cache\" % \"0.27.2\" // optional - for testing RPC services libraryDependencies += \"io.higherkindness\" %% \"mu-rpc-testing\" % \"0.27.2\" % Test sbt plugin To generate Scala code from IDL files (.proto, .avdl, OpenAPI .yaml, etc.), you will need the sbt-mu-srcgen plugin: addSbtPlugin(\"io.higherkindness\" %% \"sbt-mu-srcgen\" % \"0.27.2\")"
    } ,      
    {
      "title": "OpenAPI REST client",
      "url": "/mu-scala/tutorials/openapi-client",
      "content": "Tutorial: OpenAPI REST client This tutorial will show you how to build a REST client based on an OpenAPI (YAML) specification. This tutorial is aimed at developers who: are new to Mu-Scala have some understanding of REST APIs and OpenAPI have read the Getting Started guide Configure sbt Create a new sbt project, and add the sbt-mu-srcgen plugin in project/plugins.sbt. This plugin is going to discover and parse your OpenAPI YAML file and generate corresponding Scala code. addSbtPlugin(\"io.higherkindness\" % \"sbt-mu-srcgen\" % \"0.27.2\") Then configure the plugin by adding a few lines to build.sbt: import higherkindness.mu.rpc.srcgen.Model._ // Look for OpenAPI YAML files muSrcGenIdlType := IdlType.OpenAPI // Generate code that is compatible with http4s v0.20.x muSrcGenOpenApiHttpImpl := higherkindness.mu.rpc.srcgen.openapi.OpenApiSrcGenerator.HttpImpl.Http4sV20 The generated client will make use of http4s for the underlying HTTP client Circe for JSON serialization So we need to add the appropriate dependencies to make it compile: libraryDependencies ++= Seq( \"io.circe\" %% \"circe-core\" % \"0.12.3\", \"io.circe\" %% \"circe-generic\" % \"0.12.3\", \"org.http4s\" %% \"http4s-blaze-client\" % \"0.20.16\", \"org.http4s\" %% \"http4s-circe\" % \"0.20.16\" ) Add OpenAPI specification file Suppose you want to generate Scala code for a REST service based on the “Petstore” example OpenAPI IDL file (available for download here). Download that file and save it as src/main/resources/petstore/petstore.yaml. Generate the code You can run the source generator directly: ```shell script sbt muSrcGen or as part of compilation: ```shell script sbt compile Once the source generator has run, there should be a generated Scala file at target/scala-2.13/src_managed/main/petstore/petstore.scala. The file is very large so we won’t show it here, but it contains: case classes for all the models Circe Encoder/Decoders and http4s EntityEncoder/EntityDecoders for all models An interface for a client for the REST API: trait PetstoreClient[F[_]] { import PetstoreClient._ def getPets(limit: Option[Int], name: Option[String]): F[Pets] def createPet(newPet: NewPet): F[Either[CreatePetErrorResponse, Unit]] def getPet(petId: Int): F[Either[GetPetErrorResponse, Pet]] def deletePet(petId: Int): F[Unit] def updatePet(petId: Int, updatePet: UpdatePet): F[Unit] } An object containing factory methods to build an http4s-based client: object PetstoreHttpClient { def build[F[_]: Effect: Sync](client: Client[F], baseUrl: Uri)(implicit ...): PetstoreClient[F] = ... def apply[F[_]: Concurrent](baseUrl: Uri)(implicit ...): Resource[F, PetstoreClient[F]] = ... } Fix compilation errors There is a known issue with name clashes in the generated source. If you see compilation errors that look like this: reference to Error is ambiguous; it is both defined in object models and imported subsequently by import io.circe._ you need to manually update the Circe import from import io.circe._ to import.io.circe.{Error =&gt; _, _}. Use the client Here is an example showing how to use the generated REST client. First some imports: import petstore.models.Pets import petstore.SwaggerPetstoreClient.ListPetsErrorResponse import org.http4s._ import org.http4s.implicits._ import cats.effect.{IO, IOApp, ExitCode} import java.time.{LocalDate, LocalDateTime} import java.time.format.DateTimeFormatter import scala.concurrent.ExecutionContext.Implicits.global Then we need to define some encoders to tell the client how it should encode query parameters: trait QueryParamEncoders { def localDateTimeQueryParamEncoder(formatter: DateTimeFormatter): QueryParamEncoder[LocalDateTime] = QueryParamEncoder[String].contramap[LocalDateTime](formatter.format) def localDateQueryParamEncoder(formatter: DateTimeFormatter): QueryParamEncoder[LocalDate] = QueryParamEncoder[String].contramap[LocalDate](formatter.format) implicit val isoLocalDateTimeEncoder: QueryParamEncoder[LocalDateTime] = localDateTimeQueryParamEncoder(DateTimeFormatter.ISO_LOCAL_DATE_TIME) implicit val isoLocalDateEncoder: QueryParamEncoder[LocalDate] = localDateQueryParamEncoder(DateTimeFormatter.ISO_LOCAL_DATE) } And finally an IOApp that builds a client, uses it to hit the “list pets” endpoint and prints the response: object ClientDemo extends IOApp with QueryParamEncoders { val baseUrl = uri\"http://localhost:8080\" val clientResource = SwaggerPetstoreHttpClient[IO](baseUrl) def run(args: List[String]): IO[ExitCode] = for { response &lt;- clientResource.use(c =&gt; c.listPets(limit = Some(10))) _ &lt;- printPets(response) } yield ExitCode.Success def printPets(response: Either[ListPetsErrorResponse, Pets]): IO[Unit] = response match { case Left(error) =&gt; IO(println(s\"Received an error response! $error\")) case Right(pets) =&gt; IO(println(s\"Received a list of pets response! $pets\")) } } Working example For a full working example project including both a server and a client, check out 47deg/petstore4s."
    } ,    
    {
      "title": "Schema Evolution - Protobuf",
      "url": "/mu-scala/reference/schema-evolution/proto",
      "content": "Protocol Buffers - Schema Evolution Work in progress"
    } ,    
    {
      "title": "Generating sources from Protobuf",
      "url": "/mu-scala/guides/generate-sources-from-proto",
      "content": "Generating sources from Protocol Buffers Getting started First add the sbt plugin in project/plugins.sbt: addSbtPlugin(\"io.higherkindness\" % \"sbt-mu-srcgen\" % \"0.27.2\") NOTE For users of the sbt-mu-srcgen plugin v0.22.x and below, the plugin is enabled automatically as soon as it’s added to the project/plugins.sbt. However, for users of the sbt-mu-srcgen plugin v0.23.x and beyond, the plugin needs to be manually enabled for any module for which you want to generate code. To enable the module, add the following line to your build.sbt: enablePlugins(SrcGenPlugin) Once the plugin is enabled, you can configure it by adding a few lines to build.sbt: import higherkindness.mu.rpc.srcgen.Model._ // Look for .proto files muSrcGenIdlType := IdlType.Proto Finally, make sure you have Scala macro annotations enabled, to ensure the generated code compiles. How you do this depends on which Scala version you are using. For Scala 2.12, add this to build.sbt: addCompilerPlugin(\"org.scalamacros\" % \"paradise\" % \"2.1.1\" cross CrossVersion.patch) For Scala 2.13, add this: scalacOptions += \"-Ymacro-annotations\" Suppose you want to generate Scala code for a gRPC service based on the following Protobuf IDL file, src/main/resources/hello.proto: syntax = \"proto3\"; package foo; message HelloRequest { string arg1 = 1; string arg2 = 2; repeated string arg3 = 3; } message HelloResponse { string arg1 = 1; string arg2 = 2; repeated string arg3 = 3; } service ProtoGreeter { rpc SayHelloProto (HelloRequest) returns (HelloResponse); } You can run the source generator directly: ```shell script sbt muSrcGen or as part of compilation: ```shell script sbt compile Once the source generator has run, there should be a generated Scala file at target/scala-2.13/src_managed/main/foo/hello.scala. It will look roughly like this (tidied up and simplified for readability): package foo import higherkindness.mu.rpc.protocol._ object hello { final case class HelloRequest( arg1: String, arg2: String, arg3: List[String] ) final case class HelloResponse( arg1: String, arg2: String, arg3: List[String] ) @service(Protobuf, namespace = Some(\"foo\")) trait ProtoGreeter[F[_]] { def SayHelloProto(req: HelloRequest): F[HelloResponse] } }"
    } ,    
    {
      "title": "gRPC with Protobuf",
      "url": "/mu-scala/tutorials/service-definition/protobuf",
      "content": "Tutorial: RPC service definition with Protobuf This tutorial will show you how to generate a Mu RPC service definition from a Protocol Buffers protocol file. Then a follow-up tutorial will guide you through using this service definition to create a fully working gRPC server or client. This tutorial is aimed at developers who: are new to Mu-Scala have some understanding of Protobuf and .proto file syntax have read the Getting Started guide This document will focus on Protobuf. If you would like to use gRPC with Avro, see the RPC service definition with Avro tutorial. Create a new Mu project As described in the Getting Started guide, we recommend you use the Mu-Scala giter8 template to create a new skeleton project. This will install and configure the mu-srcgen sbt plugin, which we will need to generate Scala code from a Protobuf .proto file. When you create the project using sbt new, make sure to set create_sample_code to no. That way you can start with an empty project, and gradually fill in the implementation as you follow the tutorial. Write the Protobuf protocol We’re going to start by writing a .proto file containing a couple of messages. These messages will be used as the request and response types for a gRPC endpoint later. Copy the following Protobuf protocol and save it as protocol/src/main/resources/proto/hello.proto: syntax = \"proto3\"; package com.example; message HelloRequest { string name = 1; } message HelloResponse { string greeting = 1; bool happy = 2; } Generate Scala code Now we have a .proto file, we can generate Scala code from it. Start sbt and run the muSrcGen task. You should see some Protobuf-related log output: sbt:hello-mu-protobuf&gt; muSrcGen protoc-jar: protoc version: 3.11.1, detected platform: osx-x86_64 (mac os x/x86_64) protoc-jar: embedded: bin/3.11.1/protoc-3.11.1-osx-x86_64.exe protoc-jar: executing: [/var/folders/33/gbkw7lt97l7b38jnzh49bwvh0000gn/T/protocjar11045051115974206116/bin/protoc.exe, --proto_path=/Users/chris/code/hello-mu-protobuf/protocol/target/scala-2.13/resource_managed/main/proto/proto, --proto_path=/Users/chris/code/hello-mu-protobuf/protocol/target/scala-2.13/resource_managed/main/proto, --plugin=protoc-gen-proto2_to_proto3, --include_imports, --descriptor_set_out=hello.proto.desc, hello.proto] Let’s have a look at the code that Mu-Scala has generated. Open the file protocol/target/scala-2.13/src_managed/main/com/example/hello.scala in your editor of choice. It’s generated code, so it will look pretty ugly. Here’s a version of it tidied up a bit to make it more readable: package com.example import higherkindness.mu.rpc.protocol._ object hello { final case class HelloRequest(@pbIndex(1) name: String) final case class HelloResponse(@pbIndex(1) greeting: String, @pbIndex(2) happy: Boolean) } A few things to note: Mu-Scala has generated one case class for each Protobuf message The package name matches the one specified in the .proto file The case classes are inside an object whose name matches the filename of hello.proto Add an RPC service We now have some model classes to represent our RPC request and response, but we don’t have any RPC endpoints. Let’s fix that by adding a service to the Protobuf protocol. Add the following lines at the end of hello.proto to define an RPC service with one endpoint: service Greeter { rpc SayHello (HelloRequest) returns (HelloResponse); } Regenerate the code If you run the muSrcGen sbt task again, and inspect the protocol/target/scala-2.13/src_managed/main/com/example/hello.scala file again, it should look something like this: package com.example import higherkindness.mu.rpc.protocol._ object hello { final case class HelloRequest(@pbIndex(1) name: String) final case class HelloResponse(@pbIndex(1) greeting: String, @pbIndex(2) happy: Boolean) @service(Protobuf, namespace = Some(\"com.example\")) trait Greeter[F[_]] { def SayHello(req: HelloRequest): F[HelloResponse] } } A trait has been added to the generated code, corresponding to the service we added to hello.proto. There’s quite a lot going on there, so let’s unpack it a bit. The trait is called Greeter, which matches the service name in the .proto file. The trait contains a method for each endpoint in the service. Mu-Scala uses “tagless final” encoding: the trait has a higher-kinded type parameter F[_] and all methods return their result wrapped in F[...]. As we’ll see in a later tutorial, F[_] becomes an IO monad such as cats-effect IO when we implement a gRPC server or client. The trait is annotated with @service. This is a macro annotation. When we compile the code, it will create a companion object for the trait containing a load of useful helper methods for creating servers and clients. We’ll see how to make use of these helpers in the next tutorial. The annotation has 3 parameters: Protobuf describes how gRPC requests and responses are serialized Identity means GZip compression of requests and responses is disabled \"com.example\" is the namespace in which the RPC endpoint will be exposed These parameters can be customised using sbt settings. Take a look at the source generation reference for more details. Next steps To find out how to turn this service definition into a working gRPC client or server, continue to the gRPC server and client tutorial."
    } ,      
    {
      "title": "Source generation",
      "url": "/mu-scala/reference/source-generation",
      "content": "Source generation reference This is a reference page for the sbt-mu-srcgen sbt plugin, which generates Scala source code from Avro/Protobuf/OpenAPI IDL files. Settings This section explains each of the sbt plugin’s settings. As a reminder, this plugin needs to be manually enabled for any module for which you want to generate code; you can do that by adding the following to your build.sbt: enablePlugins(SrcGenPlugin) muSrcGenIdlType The most important sbt setting is muSrcGenIdlType, which tells the plugin what kind of IDL files (Avro/Protobuf/OpenAPI) to look for. muSrcGenIdlType := IdlType.Proto // or IdlType.Avro or IdlType.OpenAPI muSrcGenSerializationType Another important setting is muSrcGenSerializationType, which specifies how messages should be encoded on the wire. This should match the format you chose for the muSrcGenIdlType setting: For Protobuf, choose SerializationType.Protobuf For Avro, choose either SerializationType.Avro or SerializationType.AvroWithSchema. If you choose Avro, it means your client and server must always use exactly the same version of the schema. If you choose AvroWithSchema, the writer schema will be included in every message sent, which introduces a bandwidth overhead but allows schema evolution. In other words, the server and client can use different versions of a schema, as long as they are compatible with each other. See the schema evolution section for more details on schema evolution. For OpenAPI, this setting is ignored, so you don’t need to set it. muSrcGenSerializationType := SerializationType.Protobuf // or SerializationType.Avro or SerializationType.AvroWithSchema Other basic settings Setting Description Default value muSrcGenSourceDirs The list of directories where your IDL files can be found.Note: all the directories configured as sources will be distributed in the resulting jar artifact preserving the same folder structure as in the source. Compile / resourceDirectory, typically src/main/resources/ muSrcGenIdlTargetDir The directory where all discovered IDL files will be copied in preparation for Scala code generation. The plugin will automatically copy the following to the target directory: * All the IDL files and directories in the directory specified by muSrcGenSourceDirs * All the IDL files extracted from the JAR files or sbt modules specified by muSrcGenJarNames (see the “Advanced settings” section below) Compile / resourceManaged, typically target/scala-2.13/resource_managed/main muSrcGenTargetDir The directory where the muSrcGen task will write the generated files. The files will be placed in subdirectories based on the namespaces declared in the IDL files. Compile / sourceManaged, typically target/scala-2.13/src_managed/main/ Note: The directories referenced in muSrcGenSourceDirs must exist. Target directories will be created upon generation. Advanced settings Setting Description Default value muSrcGenJarNames A list of JAR file or sbt module names where extra IDL files can be found. See the srcGenJarNames section section below for more details. Nil muSrcGenIdlExtension The extension of IDL files to extract from JAR files or sbt modules. * avdl if muSrcGenIdlType is avro * proto if muSrcGenIdlType is Proto muSrcGenCompressionType The compression type that will be used by generated RPC services. Set to higherkindness.mu.rpc.srcgen.Model.GzipGen for Gzip compression. higherkindness.mu.rpc.srcgen.Model.NoCompressionGen muSrcGenIdiomaticEndpoints Flag indicating if idiomatic gRPC endpoints should be used. If true, the service operations will be prefixed by the namespace. true muSrcGenAvroGeneratorType Allows to generate Scala code either using avrohugger or skeuomorph. AvroGeneratorTypeGen.SkeumorphGen is the default; set to AvroGeneratorTypeGen.AvrohuggerGen to use the avrohugger library to generate the Scala code. AvroGeneratorTypeGen.SkeumorphGen muSrcGenStreamingImplementation Specifies whether generated Scala code will use FS2 Stream[F, A] or Monix Observable[A] as its streaming implementation. FS2 is the default; set to higherkindness.mu.rpc.srcgen.Model.MonixObservable to use Monix Observable[A] as its streaming implementation. This setting is only relevant if you have any RPC endpoint definitions that involve streaming. higherkindness.mu.rpc.srcgen.Model.Fs2Stream muSrcGenJarNames You can use IDL files packaged into artifacts within your classpath, e.g. JAR files added to the classpath via libraryDependencies, or other sbt modules. muSrcGenJarNames can be very useful when you want to distribute your IDL files without binary code (to prevent binary conflicts in clients). The following example shows how to set up a dependency with another artifact or sbt module containing the IDL definitions (foo-domain): //... .settings( Seq( muSrcGenIdlType := IdlType.Avro, muSrcGenSerializationType := SerializationType.AvroWithSchema, muSrcGenJarNames := Seq(\"foo-domain\"), muSrcGenTargetDir := (Compile / sourceManaged).value / \"compiled_avro\", libraryDependencies ++= Seq( \"io.higherkindness\" %% \"mu-rpc-service\" % V.muRPC ) ) ) //... Implementation Notes: An Intentional Incompatibility with the Avro Standard In order to make it so that it’s easier for users to evolve their schemas over time, sbt-mu-srcgen intentionally deviates from the Avro standard in one key way: it restricts RPC return types to record types (string sendUser(UserWithCountry user) is not permitted) as well as restricting the arguments of RPC messages to none or to a single record type (SendUserResponse sendUser(UserWithCountry user, RequestId id) is not permitted). If you attempt to write an Avro schema using primitive types instead of records (for example, something like this) @namespace(\"foo\") protocol UserV1 { record UserWithCountry { string name; int age; string country; } string sendUser(string user); } the source generation command (i.e. muSrcGen) will fail and return all the incompatible Avro schema records (for example, the above schema would trigger the following message: [error] (protocol / muSrcGen) One or more IDL files are invalid. Error details: [error] /path/to/the/invalid/file.avdl has the following errors: Encountered an unsupported request type: Skeuomorph only supports Record types for Avro requests. Encountered request schema with type STRING Encountered an unsupported response type: Skeuomorph only supports Record types for Avro responses. Encountered response schema with type STRING Additional Context To understand this potential issue with schema evolution, consider the following example, record SearchRequest { string query; } SearchResponse search(SearchRequest request); This schema can be evolved to add optional fields (e.g. ordering, filters, …) to the request. All the user has to do is just change the single record. This API design, on the other hand, can’t be evolved because changing the SearchResponse argument from a string to any other datatype would introduce backward incompatibility. SearchResponse search(string query); Similarly, multiple arguments don’t fully restrict API evolutions but can become inconsistent. Consider, record Filter { array&lt;string&gt; exclude; } SearchResponse search(SearchRequest query, Filter filter) If we wanted to add a way to order the results and add it to our SearchRequest, it doesn’t make sense to have filter be its own argument. For this reason, we enforce that all requests and responses must be records. Implementation note: two-stage code generation For gRPC services generated from an Avro or Protobuf definition, there are actually two stages of code generation at work. The sbt-mu-srcgen plugin will parse your IDL files and transform them into Scala code. It writes this code to .scala files under target/scala-2.13/src_managed. The generated Scala code contains @service macro annotations. When these files are compiled, the compiler will expand these annotations by executing a macro, which generates a load of boilerplate code to help with building a gRPC server or client. For example, the following .proto file: syntax = \"proto3\"; package foo.bar; message MyRequest { string a = 1; } message MyResponse { string a = 1; } service MyService { rpc MyEndpoint (MyRequest) returns (MyResponse); } would result in a .scala file that looks like (slightly simplified): package foo.bar object myproto { final case class MyRequest(a: String) final case class MyResponse(a: String) @service(Protobuf) trait MyService[F[_]] { def MyEndpoint(req: MyRequest): F[MyResponse] } } After the @service annotation is expanded at compile time, the entire generated code would look something like: package foo.bar object myproto { final case class MyRequest(a: String) final case class MyResponse(a: String) @service(Protobuf) trait MyService[F[_]] { def MyEndpoint(req: MyRequest): F[MyResponse] } object MyService { def bindService[F[_]: Concurrent]( implicit algebra: MyService[F] ): F[io.grpc.ServerServiceDefinition] = ... def client[F[_]: Concurrent]( channelFor: higherkindness.mu.rpc.ChannelFor, channelConfigList: List[higherkindness.mu.rpc.channel.ManagedChannelConfig] = List(UsePlaintext), options: io.grpc.CallOptions = io.grpc.CallOptions.DEFAULT ): Resource[F, MyService[F]] = ... def clientFromChannel[F[_]: Concurrent]( channel: F[io.grpc.ManagedChannel], options: io.grpc.CallOptions = io.grpc.CallOptions.DEFAULT ): Resource[F, MyService[F]] def unsafeClient[F[_]: Concurrent]( channelFor: higherkindness.mu.rpc.ChannelFor, channelConfigList: List[higherkindness.mu.rpc.channel.ManagedChannelConfig] = List(UsePlaintext), options: io.grpc.CallOptions = io.grpc.CallOptions.DEFAULT ): MyService[F] = ... def unsafeClientFromChannel[F[_]: Concurrent]( channel: io.grpc.ManagedChannel, options: io.grpc.CallOptions = io.grpc.CallOptions.DEFAULT ): MyService[F] } } You can see that the macro has generated a MyService companion object containing a number of helper methods for building gRPC servers and clients."
    } ,    
    {
      "title": "SSL/TLS",
      "url": "/mu-scala/guides/ssl-tls",
      "content": "SSL/TLS Encryption From the gRPC authentication guide: gRPC has SSL/TLS integration and promotes the use of SSL/TLS to authenticate the server and encrypt all the data exchanged between the client and the server. Optional mechanisms are available for clients to provide certificates for mutual authentication. Mu allows you to encrypt the connection between the server and the client through SSL/TLS. The main goal of using SSL is to protect your sensitive information and to keep your data secure between servers and clients. Netty transport Mu allows you to choose the underlying transport layer you want to use for your gRPC servers and clients: For the server you can use Netty, or the default transport provided by the gRPC Java library. (In reality the default transport will also be Netty, unless you have written your own io.grpc.ServerProvider implementation and added it to the classpath.) For the client you can use Netty or OkHttp. However, SSL/TLS encryption in Mu is currently only supported for servers and clients that use the Netty transport. Requirements On the server and client side, we will need two files to configure the SslContext in gRPC: Server/Client certificate file: Small data files that digitally bind a cryptographic key to an organization’s details. This file could be generated or obtained from a third party. Server/Client private key file: The private key is a separate file that is used in the encryption of data sent between your server and the clients. All SSL certificates require a private key to work. Usage The first step to secure our Mu services is to add the library dependencies mu-rpc-netty-ssl and mu-rpc-client-netty in our build. For the second step, we have to move both server/client certificates and private keys to a place where they can be loaded at runtime, either from the filesystem or the classpath. However, these files contain secrets, so they should not be included in the project and committed to git. If we haven’t yet generated or obtained our own certificates, we can test using certificates found here. Server side Let’s see a piece of code where we will explain line by line how to build a gRPC server with SSL encryption enabled. We won’t cover the details regarding creation of RPCService, ServerRPCService and runtime implicits. You can find more information about these in the gRPC server and client tutorial. import java.io.File import java.security.cert.X509Certificate import cats.effect.{IO, Resource} import higherkindness.mu.rpc.server.netty.SetSslContext import higherkindness.mu.rpc.server.{AddService, GrpcConfig, GrpcServer} import io.grpc.internal.testing.TestUtils import io.grpc.netty.GrpcSslContexts import io.netty.handler.ssl.{ClientAuth, SslContext, SslProvider} trait Runtime extends CommonRuntime { implicit val muRPCHandler: ServiceHandler[IO] = new ServiceHandler[IO] // Load the certicate and private key files. val serverCertFile: File = TestUtils.loadCert(\"server1.pem\") val serverPrivateKeyFile: File = TestUtils.loadCert(\"server1.key\") val serverTrustedCaCerts: Array[X509Certificate] = Array(TestUtils.loadX509Cert(\"ca.pem\")) // Build the SslContext, passing our server certificate, private key, and trusted certs. // Configure the server to use OpenSSL and require client authentication. val serverSslContext: SslContext = GrpcSslContexts .configure( GrpcSslContexts.forServer(serverCertFile, serverPrivateKeyFile), SslProvider.OPENSSL) .trustManager(serverTrustedCaCerts: _*) .clientAuth(ClientAuth.REQUIRE) .build() // Add the SslContext to the list of GrpConfigs. val grpcConfigs: Resource[IO, List[GrpcConfig]] = Greeter.bindService[IO] .map(AddService) .map(c =&gt; List(SetSslContext(serverSslContext), c)) // Important: we have to create the server with Netty. // This is the only server transport that supports SSL encryption. val server: Resource[IO, GrpcServer[IO]] = grpcConfigs.evalMap(GrpcServer.netty[IO](8080, _)) } object implicits extends Runtime Client side Similarly, let’s see how to create a gRPC client with encryption and client authentication. import higherkindness.mu.rpc.ChannelForAddress import higherkindness.mu.rpc.channel.OverrideAuthority import higherkindness.mu.rpc.channel.netty.{NettyChannelInterpreter, NettyNegotiationType, NettySslContext} import io.grpc.netty.NegotiationType object MainApp extends CommonRuntime { // Load the certicate and private key files. val clientCertChainFile: File = TestUtils.loadCert(\"client.pem\") val clientPrivateKeyFile: File = TestUtils.loadCert(\"client.key\") val clientTrustedCaCerts: Array[X509Certificate] = Array(TestUtils.loadX509Cert(\"ca.pem\")) // We have to create the SslContext for the client, like we did for the server. val clientSslContext: SslContext = GrpcSslContexts.forClient .keyManager(clientCertChainFile, clientPrivateKeyFile) .trustManager(clientTrustedCaCerts: _*) .build() // Important: the channel interpreter must be NettyChannelInterpreter. // We configure the channel interpreter to enable TLS and to use the SSL context we built. val channelInterpreter: NettyChannelInterpreter = new NettyChannelInterpreter( ChannelForAddress(\"localhost\", 8080), List(OverrideAuthority(TestUtils.TEST_SERVER_HOST)), List( NettyNegotiationType(NegotiationType.TLS), NettySslContext(clientSslContext) ) ) val muRPCServiceClient: Resource[IO, Greeter[IO]] = Greeter.clientFromChannel[IO](IO(channelInterpreter.build)) } Further reading For more details, here you can check a full explanation and an example about securing communications."
    } ,    
    {
      "title": "Testing an RPC service",
      "url": "/mu-scala/tutorials/testing-rpc-service",
      "content": "Tutorial: Testing an RPC service This tutorial will show you how to write a unit test for an RPC service using an in-memory channel and client. This tutorial is aimed at developers who: are new to Mu-Scala have read the Getting Started guide have followed the gRPC server and client tutorial Mu supports both Protobuf and Avro. For the purposes of this tutorial we will assume you are using Protobuf, but it’s possible to follow the tutorial even if you are using Avro. Service definition Let’s use the following service definition. (For an explanation of how to create service definition, check out the RPC service definition with Protobuf tutorial.) A client sends a HelloRequest containing a name, and the server responds with a greeting and an indication of whether it is feeling happy or not. import higherkindness.mu.rpc.protocol._ object hello { case class HelloRequest(@pbdirect.pbIndex(1) name: String) case class HelloResponse(@pbdirect.pbIndex(1) greeting: String, @pbdirect.pbIndex(2) happy: Boolean) // Note: the @service annotation in your code might reference Avro instead of Protobuf @service(Protobuf, namespace = Some(\"com.example\")) trait Greeter[F[_]] { def SayHello(req: HelloRequest): F[HelloResponse] } } Service implementation Here’s the implementation we want to test. import cats.Applicative import cats.syntax.applicative._ import hello._ class HappyGreeter[F[_]: Applicative] extends Greeter[F] { def SayHello(req: HelloRequest): F[HelloResponse] = HelloResponse(s\"Hello, ${req.name}!\", happy = true).pure[F] } We’re going to write a test to check that the service is always happy. cats-effect implicits In our test we’ll use cats-effect IO as our concrete effect monad. We need to provide a couple of implicits to make that work: import cats.effect.IO import scala.concurrent.ExecutionContext trait CatsEffectImplicits { import cats.effect.unsafe val EC: ExecutionContext = ExecutionContext.global implicit val ioRuntime: unsafe.IORuntime = unsafe.IORuntime.global } mu-rpc-testing You’ll need to add a dependency on the mu-rpc-testing module. This contains some helpers for setting up an in-memory service for testing. libraryDependencies += \"io.higherkindness\" %% \"mu-rpc-testing\" % \"0.27.2\" % Test Create the service and client The test will instantiate a HappyGreeter, but instead of connecting it to a real gRPC server and exposing it over HTTP, we’ll connect it to an in-memory channel. We’ll also create a client and connect it to the same in-memory channel, so it can make requests to the service. import hello._ import cats.effect.Resource import higherkindness.mu.rpc.testing.servers.withServerChannel trait ServiceAndClient extends CatsEffectImplicits { implicit val greeter: Greeter[IO] = new HappyGreeter[IO] /* * A cats-effect Resource that builds a gRPC server and client * connected to each other via an in-memory channel. */ val clientResource: Resource[IO, Greeter[IO]] = for { sc &lt;- withServerChannel(Greeter.bindService[IO]) clientRes &lt;- Greeter.clientFromChannel[IO](IO.pure(sc.channel)) } yield clientRes } The important part here is the use of withServerChannel. This is a helper method provided by the mu-rpc-testing module that connects the service to an in-memory channel so we don’t need to start a real gRPC server. Write the test Now we’re ready to write our test. With the service and client in place, the test consists of using the client to make a request and then asserting that the response matches what we expect. import hello._ import org.scalatest.flatspec.AnyFlatSpec class ServiceSpec extends AnyFlatSpec with ServiceAndClient { behavior of \"Greeter service\" it should \"be happy\" in { val response: HelloResponse = clientResource .use(client =&gt; client.SayHello(HelloRequest(\"somebody\"))) .unsafeRunSync() assert(response.happy === true) } } Run the test Let’s see the test in action: org.scalatest.nocolor.run(new ServiceSpec) // MdocSession$App$ServiceSpec: // Greeter service // - should be happy Bonus points: property-based test Since the HappyGreeter service is always happy, we could also write this as a property-based test with ScalaCheck to verify that the service’s happiness does not depend on the incoming request. import org.scalatestplus.scalacheck.Checkers import org.scalacheck.Gen import org.scalacheck.Prop._ class PropertyBasedServiceSpec extends AnyFlatSpec with ServiceAndClient with Checkers { val requestGen: Gen[HelloRequest] = Gen.alphaStr.map(HelloRequest) behavior of \"Greeter service\" it should \"be happy\" in { clientResource.use { client =&gt; IO { check { forAll(requestGen) { request =&gt; val response: HelloResponse = client.SayHello(request).unsafeRunSync() response.happy :| \"response should be happy\" } } } }.unsafeRunSync() } } Let’s run this test as well: org.scalatest.nocolor.run(new PropertyBasedServiceSpec) // MdocSession$App$PropertyBasedServiceSpec: // Greeter service // - should be happy"
    }    
  ];

  idx = lunr(function () {
    this.ref("title");
    this.field("content");

    docs.forEach(function (doc) {
      this.add(doc);
    }, this);
  });

  docs.forEach(function (doc) {
    docMap.set(doc.title, doc.url);
  });
}

// The onkeypress handler for search functionality
function searchOnKeyDown(e) {
  const keyCode = e.keyCode;
  const parent = e.target.parentElement;
  const isSearchBar = e.target.id === "search-bar";
  const isSearchResult = parent ? parent.id.startsWith("result-") : false;
  const isSearchBarOrResult = isSearchBar || isSearchResult;

  if (keyCode === 40 && isSearchBarOrResult) {
    // On 'down', try to navigate down the search results
    e.preventDefault();
    e.stopPropagation();
    selectDown(e);
  } else if (keyCode === 38 && isSearchBarOrResult) {
    // On 'up', try to navigate up the search results
    e.preventDefault();
    e.stopPropagation();
    selectUp(e);
  } else if (keyCode === 27 && isSearchBarOrResult) {
    // On 'ESC', close the search dropdown
    e.preventDefault();
    e.stopPropagation();
    closeDropdownSearch(e);
  }
}

// Search is only done on key-up so that the search terms are properly propagated
function searchOnKeyUp(e) {
  // Filter out up, down, esc keys
  const keyCode = e.keyCode;
  const cannotBe = [40, 38, 27];
  const isSearchBar = e.target.id === "search-bar";
  const keyIsNotWrong = !cannotBe.includes(keyCode);
  if (isSearchBar && keyIsNotWrong) {
    // Try to run a search
    runSearch(e);
  }
}

// Move the cursor up the search list
function selectUp(e) {
  if (e.target.parentElement.id.startsWith("result-")) {
    const index = parseInt(e.target.parentElement.id.substring(7));
    if (!isNaN(index) && (index > 0)) {
      const nextIndexStr = "result-" + (index - 1);
      const querySel = "li[id$='" + nextIndexStr + "'";
      const nextResult = document.querySelector(querySel);
      if (nextResult) {
        nextResult.firstChild.focus();
      }
    }
  }
}

// Move the cursor down the search list
function selectDown(e) {
  if (e.target.id === "search-bar") {
    const firstResult = document.querySelector("li[id$='result-0']");
    if (firstResult) {
      firstResult.firstChild.focus();
    }
  } else if (e.target.parentElement.id.startsWith("result-")) {
    const index = parseInt(e.target.parentElement.id.substring(7));
    if (!isNaN(index)) {
      const nextIndexStr = "result-" + (index + 1);
      const querySel = "li[id$='" + nextIndexStr + "'";
      const nextResult = document.querySelector(querySel);
      if (nextResult) {
        nextResult.firstChild.focus();
      }
    }
  }
}

// Search for whatever the user has typed so far
function runSearch(e) {
  if (e.target.value === "") {
    // On empty string, remove all search results
    // Otherwise this may show all results as everything is a "match"
    applySearchResults([]);
  } else {
    const tokens = e.target.value.split(" ");
    const moddedTokens = tokens.map(function (token) {
      // "*" + token + "*"
      return token;
    })
    const searchTerm = moddedTokens.join(" ");
    const searchResults = idx.search(searchTerm);
    const mapResults = searchResults.map(function (result) {
      const resultUrl = docMap.get(result.ref);
      return { name: result.ref, url: resultUrl };
    })

    applySearchResults(mapResults);
  }

}

// After a search, modify the search dropdown to contain the search results
function applySearchResults(results) {
  const dropdown = document.querySelector("div[id$='search-dropdown'] > .dropdown-content.show");
  if (dropdown) {
    //Remove each child
    while (dropdown.firstChild) {
      dropdown.removeChild(dropdown.firstChild);
    }

    //Add each result as an element in the list
    results.forEach(function (result, i) {
      const elem = document.createElement("li");
      elem.setAttribute("class", "dropdown-item");
      elem.setAttribute("id", "result-" + i);

      const elemLink = document.createElement("a");
      elemLink.setAttribute("title", result.name);
      elemLink.setAttribute("href", result.url);
      elemLink.setAttribute("class", "dropdown-item-link");

      const elemLinkText = document.createElement("span");
      elemLinkText.setAttribute("class", "dropdown-item-link-text");
      elemLinkText.innerHTML = result.name;

      elemLink.appendChild(elemLinkText);
      elem.appendChild(elemLink);
      dropdown.appendChild(elem);
    });
  }
}

// Close the dropdown if the user clicks (only) outside of it
function closeDropdownSearch(e) {
  // Check if where we're clicking is the search dropdown
  if (e.target.id !== "search-bar") {
    const dropdown = document.querySelector("div[id$='search-dropdown'] > .dropdown-content.show");
    if (dropdown) {
      dropdown.classList.remove("show");
      document.documentElement.removeEventListener("click", closeDropdownSearch);
    }
  }
}
